{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6008cf75-5b8f-4f34-a36a-e200dfa1f82f",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45f246a-51d5-4c91-a09a-d8ecf1945237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4247/3638372330.py:13: DtypeWarning: Columns (10,63,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_all = pd.read_csv(\"results/predictions_all_test.csv\")\n"
     ]
    }
   ],
   "source": [
    "### Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "classes = [\"ABCA4\", \"BBS1\", \"BEST1\", \"CACNA1F\", \"CDH23\", \"CERKL\", \"CHM\", \"CNGA3\", \"CNGB3\",\n",
    "           \"CRB1\", \"CRX\", \"CYP4V2\", \"EFEMP1\", \"EYS\", \"GUCY2D\", \"KCNV2\", \"MERTK\", \"MTTL1\",\n",
    "           \"MYO7A\", \"NR2E3\", \"OPA1\", \"PDE6B\", \"PROML1\", \"PRPF31\", \"PRPF8\", \"PRPH2\", \"RDH12\",\n",
    "           \"RHO\", \"RP1\", \"RP1L1\", \"RP2\", \"RPE65\", \"RPGR\", \"RS1\", \"TIMP3\", \"USH2A\"]\n",
    "\n",
    "# Test data\n",
    "# N.B: This is only 1st appointment test data\n",
    "df_all = pd.read_csv(\"results/predictions_all_test.csv\")\n",
    "df_external = df_all[df_all['hospital'] != \"Moorfields\"]\n",
    "df_mf_test = df_all[df_all['hospital'] == \"Moorfields\"] \n",
    "\n",
    "# Moorfields test + CV\n",
    "# This is all the moorfields predictions in subsequent appointments + Cross-validation predictions\n",
    "df_mf = pd.read_csv(\"results/predictions_moorfields.csv\") \n",
    "df_mf_cv = df_mf[df_mf['fold'] > -1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe8c49-a8df-43eb-a9f4-bb537ea95477",
   "metadata": {},
   "source": [
    "### Metrics and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822fc145-a012-4078-9d38-d51c1b7307de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouby patient number\n",
    "# and then group by another value and accregate predictions\n",
    "def get_preds(df, group_on='file.path'):\n",
    "    pandas_query = [ 'pred.'+cls for cls in classes ]\n",
    "    truth, pred_scores, pat_ids = list(), list(), list()\n",
    "    for pat_id, pat_entries in df.groupby('patient.number'):\n",
    "        gene = pat_entries['gene'].values[0]\n",
    "        gene_ind = classes.index(gene)\n",
    "        for val, grouped_entries in pat_entries.groupby(group_on):\n",
    "            pred_scores.append(grouped_entries[pandas_query].mean(axis=0))\n",
    "            pat_ids.append(pat_id)\n",
    "            truth.append(gene_ind)   \n",
    "    return np.array(truth), np.array(pred_scores), pat_ids\n",
    "\n",
    "# Metrics\n",
    "\n",
    "def accuracy(truth, pred_scores):\n",
    "    pred_class = np.argmax(pred_scores, axis=-1)\n",
    "    correct = pred_class == truth\n",
    "    return correct.mean()\n",
    "\n",
    "def top_k(truth, pred_scores, k):\n",
    "    rank = len(classes) - np.argwhere(np.argsort(pred_scores) == truth[:,np.newaxis])[:,1] # 'rank' of correct prediction\n",
    "    return np.mean(rank <= k)\n",
    "\n",
    "# The below section throws up a bunch of numpy warnings - these are handled later so redundant\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "def per_class_stats(truth, pred_scores, class_ind):\n",
    "    class_true = truth == class_ind\n",
    "    class_prediction = pred_scores[:, class_ind]\n",
    "    class_predicted = pred_scores.argmax(axis=-1) == class_ind\n",
    "    \n",
    "    tp = class_true & class_predicted\n",
    "    tn = ~ (class_true | class_predicted)\n",
    "    \n",
    "    precision = tp.sum() / class_predicted.sum()\n",
    "    recall = tp.sum() / class_true.sum() # N.B: This is also Sensitivity\n",
    "    specificity = tn.sum() / (1-class_true).sum()\n",
    "\n",
    "    # Calculate Precision-Recall + ROC curves\n",
    "    if class_true.any() and not class_true.all():\n",
    "        pr_auc  = sklearn.metrics.average_precision_score(class_true, class_prediction, average=\"macro\")\n",
    "        #This is giving a different value to AUC of np PRC\n",
    "        # Apparently average_precision_score calculates the mean Precision-Recall curve then takes the AUC,\n",
    "        # while for mean AUC we calculate the AUPRC for each class then mean.\n",
    "        roc_auc = sklearn.metrics.roc_auc_score(class_true, class_prediction)\n",
    "    else:\n",
    "        pr_auc, roc_auc = np.NAN, np.NAN\n",
    "    \n",
    "    return precision, recall, specificity, pr_auc, roc_auc\n",
    "\n",
    "def mean_per_class_stats(truth, pred_scores):\n",
    "    return np.nanmean([ per_class_stats(truth, pred_scores, i) for i in range(pred_scores.shape[-1]) ], axis=0)\n",
    "\n",
    "def get_bootstrap_samples(metric_fn, truth, pred_scores, n_samples=1000, sample_size=None, n_workers=None):\n",
    "    sample_size = sample_size if sample_size else len(truth)\n",
    "    indices = np.random.choice(len(truth), size=(n_samples,sample_size))\n",
    "    if n_workers:\n",
    "        from multiprocessing import Pool\n",
    "        with Pool(n_workers) as pool:\n",
    "            result = list(pool.starmap(metric_fn, zip(truth[indices], pred_scores[indices])))\n",
    "    else:\n",
    "        result = list(map(metric_fn, truth[indices], pred_scores[indices]))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3f6c8-4986-4816-83de-76e7b709c99a",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0eaf51-7ca9-4c4a-84aa-b157fe837dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(truth, pred_scores, bootstrapped=False):\n",
    "    n_classes = pred_scores.shape[-1]\n",
    "    \n",
    "    # Accuracy and top-k\n",
    "    pred_class = np.argmax(pred_scores, axis=-1)\n",
    "    correct = pred_class == truth\n",
    "    rank = n_classes - np.argwhere(np.argsort(pred_scores) == truth[:,np.newaxis])[:,1] # 'rank' of correct prediction\n",
    "\n",
    "    print(\"Accuracy: {:2.1f}%\".format(correct.mean() * 100))\n",
    "    for i in [2,3,5,10,20,36]:\n",
    "        print(\"top-{}: {:2.1f}%\".format(i, np.mean(rank <= i) * 100))\n",
    "    \n",
    "    aucs = list()\n",
    "    prcs_aucs = list()\n",
    "    for i in range(n_classes):\n",
    "        class_true = truth == i\n",
    "        class_prediction = pred_scores[:,i]\n",
    "\n",
    "        fpr, tpr, threshold = sklearn.metrics.roc_curve(class_true, class_prediction)\n",
    "        aucs.append(sklearn.metrics.auc(fpr, tpr))\n",
    "        pr, rec, threshold = sklearn.metrics.precision_recall_curve(class_true, class_prediction)\n",
    "        prcs_aucs.append(sklearn.metrics.auc(rec, pr))\n",
    "    print(\"Mean AUROC: {:0.3f}\".format(np.nanmean(aucs)))\n",
    "    print(\"Mean AUPRC: {:0.3f} (trapezoidal)\".format(np.nanmean(prcs_aucs)))\n",
    "    \n",
    "    if bootstrapped:\n",
    "        # Confidence intervals\n",
    "        bootstrap_accuracy = get_bootstrap_samples(accuracy, truth, pred_scores, n_samples=10000)\n",
    "        acc_mean = bootstrap_accuracy.mean(axis=0) * 100\n",
    "        acc_upper = np.percentile(bootstrap_accuracy, 97.5) * 100\n",
    "        acc_lower = np.percentile(bootstrap_accuracy, 2.5) * 100\n",
    "        print(\"Bootstrapped Accuracy: {:2.1f}% ({:2.1f}-{:2.1f}%)\".format(acc_mean,acc_lower,acc_upper))\n",
    "\n",
    "        bootstrap_per_class = get_bootstrap_samples(mean_per_class_stats, truth, pred_scores, n_samples=1000, n_workers=100)\n",
    "        # precision, recall, specificity, pr_auc, roc_auc\n",
    "        pc_mean = bootstrap_per_class.mean(axis=0)\n",
    "        pc_interval = np.percentile(bootstrap_per_class, [2.5,97.5], axis=0) \n",
    "        print(\"Bootstrapped AUROC: {:1.3f} ({:1.3f}-{:1.3f})\".format(pc_mean[4], pc_interval[0,4], pc_interval[1,4]))\n",
    "        print(\"Bootstrapped AUPRC: {:1.3f} ({:1.3f}-{:1.3f}) (average precision)\".format(pc_mean[3], pc_interval[0,3], pc_interval[1,3]))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf26852-6852-45a4-bc0d-ea15c0fd8278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal test set\n",
      "Accuracy: 66.7%\n",
      "top-2: 75.4%\n",
      "top-3: 80.7%\n",
      "top-5: 85.6%\n",
      "top-10: 94.7%\n",
      "top-20: 98.9%\n",
      "top-36: 100.0%\n",
      "Mean AUROC: 0.935\n",
      "Mean AUPRC: 0.564 (trapezoidal)\n",
      "Bootstrapped Accuracy: 66.7% (61.0-72.3%)\n",
      "Bootstrapped AUROC: 0.936 (0.919-0.955)\n",
      "Bootstrapped AUPRC: 0.602 (0.528-0.678) (average precision)\n",
      "\n",
      "CPU times: user 1.47 s, sys: 625 ms, total: 2.09 s\n",
      "Wall time: 7.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Internal test set (Moorfields)\n",
    "df = df_mf_test[df_mf_test['gene'].isin(classes)]\n",
    "truth, pred_scores, pat_ids = get_preds(df, group_on='patient.number')\n",
    "\n",
    "print(\"Internal test set\")\n",
    "print_results(truth, pred_scores, bootstrapped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14939508-e861-4dc7-b89e-9a3cbe7365bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External test set\n",
      "Accuracy: 65.3%\n",
      "top-2: 74.2%\n",
      "top-3: 80.9%\n",
      "top-5: 86.4%\n",
      "top-10: 93.2%\n",
      "top-20: 98.3%\n",
      "top-36: 100.0%\n",
      "Mean AUROC: 0.930\n",
      "Mean AUPRC: 0.442 (trapezoidal)\n",
      "Bootstrapped Accuracy: 65.2% (59.3-71.2%)\n",
      "Bootstrapped AUROC: 0.928 (0.902-0.951)\n",
      "Bootstrapped AUPRC: 0.540 (0.465-0.612) (average precision)\n",
      "\n",
      "CPU times: user 1.32 s, sys: 717 ms, total: 2.04 s\n",
      "Wall time: 8.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# External test set (Liverpool + Oxford + Bonn + Sao Paulo)\n",
    "df = df_external[df_external['gene'].isin(classes)]\n",
    "truth, pred_scores, pat_ids = get_preds(df, group_on='patient.number')\n",
    "\n",
    "print(\"External test set\")\n",
    "print_results(truth, pred_scores, bootstrapped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0716c0f9-9feb-4b15-b0f4-552018fdc34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test data\n",
      "Accuracy: 66.0%\n",
      "top-2: 74.8%\n",
      "top-3: 80.8%\n",
      "top-5: 86.0%\n",
      "top-10: 94.0%\n",
      "top-20: 98.6%\n",
      "top-36: 100.0%\n",
      "Mean AUROC: 0.932\n",
      "Mean AUPRC: 0.449 (trapezoidal)\n",
      "Bootstrapped Accuracy: 66.0% (61.8-70.0%)\n",
      "Bootstrapped AUROC: 0.933 (0.917-0.947)\n",
      "Bootstrapped AUPRC: 0.504 (0.441-0.572) (average precision)\n",
      "\n",
      "CPU times: user 1.74 s, sys: 3.88 s, total: 5.62 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Combined\n",
    "df = df_all[df_all['gene'].isin(classes)]\n",
    "truth, pred_scores, pat_ids = get_preds(df, group_on='patient.number')\n",
    "\n",
    "print(\"All test data\")\n",
    "print_results(truth, pred_scores, bootstrapped=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b901d-e1bd-4354-8fa7-4e36bf7e6fee",
   "metadata": {},
   "source": [
    "### By individual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38755c0e-59dd-4f93-a7ab-c662151bce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names =[{'BAF': 'models_baf/17012021-173755-InceptionV3-100e-128bs-0.0001lr.h5',\n",
    "               'OCT': 'models_oct/18012021-095554-InceptionV3-100e-128bs-0.0001lr.h5',\n",
    "               'IR':  'models_ir/18012021-203445-InceptionV3-100e-128bs-0.0001lr.h5'},\n",
    "              {'BAF': 'models_baf/17012021-202904-InceptionV3-100e-128bs-0.0001lr.h5',\n",
    "               'OCT': 'models_oct/18012021-120608-InceptionV3-100e-128bs-0.0001lr.h5',\n",
    "               'IR':  'models_ir/19012021-010048-InceptionV3-100e-128bs-0.0001lr.h5'},\n",
    "              {'BAF': 'models_baf/17012021-232108-InceptionV3-100e-128bs-0.0001lr.h5',\n",
    "               'OCT': 'models_oct/18012021-141228-InceptionV3-100e-128bs-0.0001lr.h5',\n",
    "               'IR':  'models_ir/19012021-051220-InceptionV3-100e-128bs-0.0001lr.h5'},\n",
    "              {'BAF': 'models_baf/18012021-023016-InceptionV3-100e-128bs-0.0001lr.h5',\n",
    "               'OCT': 'models_oct/18012021-161956-InceptionV3-100e-128bs-0.0001lr.h5', \n",
    "               'IR':  'models_ir/19012021-091140-InceptionV3-100e-128bs-0.0001lr.h5'}, \n",
    "              {'BAF': 'models_baf/18012021-052425-InceptionV3-100e-128bs-0.0001lr.h5', \n",
    "               'OCT': 'models_oct/18012021-182559-InceptionV3-100e-128bs-0.0001lr.h5', \n",
    "               'IR':  'models_ir/19012021-131130-InceptionV3-100e-128bs-0.0001lr.h5'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7436842-0870-43de-a881-a775a99d68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test = df_mf_test[df_mf_test['gene'].isin(classes)]\n",
    "\n",
    "print(\"Test set results:\")\n",
    "\n",
    "for modality in [\"BAF\", \"IR\", \"OCT\" ]:\n",
    "    #df_modality = df_test[df_test[\"modality\"] == modality]\n",
    "    for fold in range(5):\n",
    "        \n",
    "        df = df_test[df_test['pred.model'] == model_names[fold][modality]]\n",
    "        truth, pred_scores, pat_ids = get_preds(df, group_on='file.path') # filter of filepath for individual results\n",
    "\n",
    "        print(modality, \"model\", str(fold+1))\n",
    "        print(model_names[fold][modality])\n",
    "        print_results(truth, pred_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce8684-3377-4831-a682-ebadbf074484",
   "metadata": {},
   "source": [
    "## Restricted to certain classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11de20cc-a963-4058-b7d1-17f2bcdbc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_classes(labels, preds, subclasses):\n",
    "    subclass_idxs = [ classes.index(cls) for cls in subclasses ]\n",
    "    subclass_newidxs = [ subclasses.index(cls) for cls in subclasses ]\n",
    "    idx_map = dict(zip(subclass_idxs, subclass_newidxs))\n",
    "    rws = np.isin(labels, subclass_idxs)\n",
    "    preds = preds[rws][:, subclass_idxs]\n",
    "    labels = np.vectorize(idx_map.get)(labels[rws])\n",
    "    return labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd1dacc-86ee-4976-8b2b-ba7960ced8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740157480314961\n",
      "Accuracy: 87.4%\n",
      "top-2: 99.2%\n",
      "top-3: 100.0%\n",
      "top-5: 100.0%\n",
      "top-10: 100.0%\n",
      "top-20: 100.0%\n",
      "top-36: 100.0%\n",
      "Mean AUROC: 0.940\n",
      "Mean AUPRC: 0.871 (trapezoidal)\n",
      "Bootstrapped Accuracy: 87.4% (81.1-92.9%)\n",
      "Bootstrapped AUROC: 0.940 (0.899-0.973)\n",
      "Bootstrapped AUPRC: 0.875 (0.801-0.938) (average precision)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_mf_test[df_mf_test['gene'].isin(classes)]\n",
    "truth, pred_scores, pat_ids = get_preds(df, group_on='patient.number')\n",
    "\n",
    "truth, pred_scores = restrict_classes(truth, pred_scores, [\"ABCA4\", \"USH2A\"])\n",
    "\n",
    "print_results(truth, pred_scores, bootstrapped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83972e2f-e871-49d9-9d11-2b8813497294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
